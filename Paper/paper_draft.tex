\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage[final, nonatbib]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts, amssymb, amsmath}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage{csvsimple}
\usepackage{subcaption}
\usepackage{numprint}
\usepackage{wrapfig}


\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bbeta}{\mathbf{\beta}}



\graphicspath{ {images/} }

\title{Survival of the Fittest: Variable Selection on Agricultural Data from the Gal\'apagos Islands}

\author{
  Michael Bostwick\\
  Department of Statistics and Operations Research\\
  University of North Carolina at Chapel Hill\\
	\\
  \bf{Client: Francisco Laso} \\
  Department of Geography \\
  University of North Carolina at Chapel Hill\\
  \\
  March 27th, 2018 \\
}

\begin{document}

\maketitle
\begin{abstract}
  Variable selection is an important first step when analyzing datasets with a large number of potential predictor variables. We 
  apply two techniques, Forward Selection and Elastic Net,  to find the most important variables in a dataset detailing over 200 
  socioeconomic measurements for 755 farms on the Gal\'apagos Islands. Modeling five different outcome variables, we find 
  the data available has the strongest linear relationships with the outcomes \textit{productivity} and \textit{land use choices}. For each of 
  the outcome variables we present the top five predictor variables as well as a full set of coefficients for the optimally predictive 
  model.
  \end{abstract}

\section{Introduction}

\subsection{Background}

The Gal\'apagos Islands make for a feasible and significant case study of complex systems. Due to their relative isolation and smaller size, the interaction of factors can more realistically be modeled for the Gal\'apagos Islands than other systems. Yet, the Gal\'apagos Islands also represents an important example of the competing forces of resource conservation and economic development in a rapidly changing environment. Prior work has created agent-based simulation models of the Gal\'apagos, but with limited interaction parameters between agents, particularly in regards to farm success (\cite{miller}, \cite{valdivia},\cite{walsh}). In order to create a more detailed and perhaps more accurate simulation, the relationships between different factors on the island must be better understood. This work aims to search through a large number of possible relationships and identify the empirically most significant ones for future study and incorporation into simulation models. 

\subsection{Data}

The data available to study the dynamics between agricultural measures and related factors primarily 
comes from the Censo de las Unidades de Producci\'on Agropecuaria (UPA) de Gal\'apagos (Census of Agricultural Production 
Units (UPA) of Galapagos) (\cite{census}). This is a self-reported survey with data from 755 farms (UPAs) detailing many characteristics. The questions covered fall into the following categories:
\begin{itemize}
\item{General characteristics (land area, age of landowner, etc.)}
\item{Permanent crops (specific types and quantity)}
\item{Temporary crops (specific types and quantity)}
\item{Pastures (specific types and quantity)}
\item{Tree crops (specific types and quantity)}
\item{Livestock production (detailed by animal)}
\item{Expenses (detailed by category)}
\item{Workers (detailed by role)}
\item{Land use (8 different categories)}

\end{itemize}

In addition to this census, data is also available from satellite image classification including 
information on water, energy and road access. In total, under the direction of the client, 239 variables were selected for 
consideration in modeling relationships between potential predictors and five outcome variables of interest. 

Some of the five outcome variables of interest came directly from survey responses, while others were derived from a 
combination of multiple variables. When analyzing a derived outcome variable all variables used in its original calculation were 
removed from consideration in the model. The client also denoted specific predictor variables to exclude from particular 
models when their inclusion would not be beneficial. For example, while the 
amount of crops sold in pounds was not directly used to calculate \textit{net income}, the obvious relationship existent 
precluded it from inclusion. In addition, predictor variables that met one or more of the following criteria were removed prior to 
modeling: zero variance, extremely high (>0.99) or perfect correlation with other predictor variables, or linear dependence with 
other predictor variables (that is, two or more predictor variables could be linearly combined to create another predictor variable). The 
exact number of predictor variables included in each model varied slightly, but there were approximately 200 predictors variables examined for each model after the preceding steps were taken.

\subsection{Organization of Report}

The remainder of this report is divided into four sections. Section 2 provides a brief overview of the analysis so that the results can be understood. In Section 3 results for each of the 5 outcome variables are provided, where standard tables and graphs are repeated for each. A more in-depth explanation of the statistical methods used is contained in Section 4, but this section can be referenced as needed. Section 5 details important considerations when interpreting the results and suggests possible avenues for future work. Lastly, References and the Appendix, including additional tables and figures, can be found following Section 5.

\section{Modeling}
\subsection{Challenges to address}

The primary challenge in this analysis is the vast number of potential predictor variables. This challenge is twofold; 1.) when 
the number of predictors is large the determination of a reliable model is difficult and 2.) interpreting the coefficients of many 
predictors simultaneously is not an easy task for humans (and will make resulting simulations overly complicated). For this 
reason, the analysis focuses on the use of two variable selection techniques that aim to build a linear model with a subset of 
the available variables that still maintains a strong explanatory/predictive performance.

Secondarily, when performing standard linear regression the error is assumed to be normally distributed.  While this does not 
mean the outcome variable necessarily needs to be normally distributed, large deviations from normality can cause issues. 
Several outcome variables in this study show strong non-normality, which can contribute to a poorly fitting model and unreliable estimates of the coefficients. In order to address this issue transformations to the data and modifications to the standard linear model were considered.

\subsection{Overview of methods}

A summary of the statistical methods used is presented here to allow for understanding of results. For further details see 
Section 4: Statistical Methods. For each of the outcome variables of interest a set of linear models using the appropriate subset 
of predictors was built. Each relationship was modeled 
using Forward Selection and Elastic Net regression. Forward Selection fits a linear model by progressively adding variables to 
the model until a best fit is found. This results in only some of the variables being included, chosen in a discrete manner 
(computed using the R package `leaps' \cite{lumley}). 
Elastic Net regression fits a linear model by limiting the size of the coefficients so that they are smaller than in standard least 
squares, and for many variables actually shrunken to zero. Similar to Forward Selection this results in a smaller model, but 
variable selection can be carefully tuned as optimization is done in a more continuous way (computed using the R package `glmnet' \cite{friedman}).

In general, these techniques have slightly different aims. Forward Selection chooses a model that best explains the variance in 
the dataset at hand. Elastic Net chooses a model that can best make predictions on new data. Depending on the goals of 
analysis, one technique is not necessarily better so we do not compare the two quantitatively, but instead offer both results as 
varying perspectives on variable selection. While a variable being chosen by both methods provides stronger evidence that an 
important relationship exists, disagreement suggests exploring both possibilities instead of one method necessarily being 
incorrect.

\section{Results}

The Results section is broken into three subsections covering the outcome categories of interest: farm success, invasive species and land use choices. Within farm success we further divide into three specific measurements, resulting in a total of 5 different outcome variables. We explore each outcome variable in turn, presenting a visualization of the data, the top five predictor variables and analysis of optimal linear models.

\subsection{Farm Success}

The first three outcome variables of interest can be grouped together under the category of farm success:
\begin{itemize}
	\item \textit{Productivity}: total pounds of crops and livestock produced divided by the farm surface area. 	
	\item \textit{Net income}: revenue from all products sold minus total expenses.
	\item \textit{Number of workers supported}: total labor expenditures divided by a standard 
	full-time worker's salary.
\end{itemize}

\subsubsection{Productivity}

\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{production_histograms.pdf}
\setlength{\belowcaptionskip}{-10pt}
\caption{\textsl{\small Histogram of Productivity, from left to right showing the full range, zoomed into 0-10,000 lbs/hectare, and the log transformed nonzero values. The log transformed values exhibit the desired normal shape.}}
\label{figure:prod_hist}
\end{figure}

The histograms of the \textit{Productivity} variable (Figure \ref{figure:prod_hist}) show a strong skewness, both when looking at 
all observations, and when zooming into observations between 0-10,000 lbs/hectare. To achieve a distribution closer to normal 
(bell-curved), which will benefit the linear model, we took the $log_{10}$ transformation with resulting data shown in the last 
plot. Since the log transformation cannot be performed on zero values, we removed the 38 occasions of this from the dataset. 
Beyond the mathematical constraint, farms with zero production perhaps are not farms as typically defined.

We built linear models using both methods, Elastic Net and Forward Selection, on the log-transformed 
\textit{Productivity} variable, recording an optimal model of any size and the best 5 variable model for each. The size of 5 variables is chosen to provide quick insight and not based on any specific statistical property. The results of the best 5 variable model are shown 
in Table \ref{table:prod_top5}, listed in order of entrance into the model.  Next to variable names the direction of the relationship is indicated 
with a (+) or (-). 

\begin{table}[h]
\centering
\setlength{\tabcolsep}{20pt}
\begin{tabular}{cc}
\bfseries \underline{Elastic Net} & \bfseries \underline{Forward~Selection} 
\csvreader[head to column names]{production_top5names.csv}{}%
{\\\elasticnet & \forward}%
\end{tabular}
\caption{\textsl{\small Modeling of Productivity, Top 5 features for both methods}}
\label{table:prod_top5}
\end{table}

Plots from the optimal models for Elastic Net and Forward Selection are shown in Figures \ref{figure:prod_opt_e} and  \ref{figure:prod_opt_f}. The cross-
validation plot for Elastic Net can be understood as follows: the 
horizontal axis shows the number of variables included in the model (on top) and the corresponding lambda ($\lambda$) value 
(on the bottom), the vertical axis shows the Mean-Squared Error (MSE) represented as the red dots and surrounded by bars 
showing the standard deviation. The vertical dashed line to the left, $\lambda_{min}$, is found at the minimum MSE and the 
vertical dashed line to the right, $\lambda_{1se}$ is at the largest lambda within one standard error of the minimum. The idea 
behind $\lambda_{1se}$ is that similar error performance can be achieved with a smaller model, in this case a model with 40 
fewer variables. Since our goal is to select a small amount of variables, we will generally use the model found at $
\lambda_{1se}$. 

\begin{wrapfigure}{l}{0.6\textwidth}
\includegraphics[width=0.6\textwidth]{elastic_cv_production.pdf}
\setlength{\belowcaptionskip}{-10pt}
\caption{\textsl{\small Elastic Net Cross-validation plot for Productivity. The plot shows a desirable "U" shape where a moderate number of variables provides a significant improvement in prediction error.}}
\label{figure:prod_opt_e}
\end{wrapfigure}


For Elastic Net we measure performance with Root Mean-Squared Error (RMSE), which is on average how far the predicted 
value is from the actual value across cross-validation runs. The 5 variable Elastic Net model had a RMSE of 0.78. Not counting 
the intercept term, the optimal model chosen for Elastic Net included 45 variables and had a RMSE of 0.61. To provide context 
for the RMSE, we can look at the point furthest to the right on the cross-validation plot in Figure \ref{figure:prod_opt_e} and see 
how the model would perform when no variables are included in the model, that is, just predicting the average outcome value.

For Forward Selection the plot, Figure \ref{figure:prod_opt_f}, is much more straightforward. We plot the number of variables 
included versus the Bayesian Information Criterion (BIC) that we choose to minimize, and mark the optimal point in red. The 
convex shape of the plots highlights a common trend in variable selection; not including enough variables does not provide 
enough information, but beyond a certain point adding more variables may not be worth the added complexity. 


\begin{wrapfigure}{r}{0.55\textwidth}
\vspace*{-0.5cm}
\includegraphics[width=0.55\textwidth]{forward_nvars_production.pdf}
\setlength{\belowcaptionskip}{-20pt}
\caption{\textsl{\small Forward Selection plot for Productivity. The optimal model includes 26 variables.}}
\label{figure:prod_opt_f}
\end{wrapfigure}



We evaluate Forward Selection with $R^2$, which is the percentage of variability in the outcome variable that is explained by 
the model. The 5 variable Forward Selection model had an $R^2$ of 0.48 and the full model included 26 variables and had a 
$R^2$ of 0.63. 

The coefficients estimated for both full models can be found in Table \ref{table:prod_full} in the Appendix. These numbers 
suggest that while the 5 variable model is helpful, there is a decent amount of information to be 
gained by adding more variables. Diagnostics of the linear fit of the optimal Elastic Net and Forward Selection models (plots 
shown in Figure \ref{figure:resids_prod} in the Appendix) do not raise any concerns. 


\subsubsection{Net Income}

The histograms for \textit{Net Income} (Figure \ref{figure:netincome_hist}) show a symmetric shape, but a very spiky center and 
a few observations wide in the tails. We attempted to fit a model on the full dataset, but find the observations with large absolute values are obscuring other possible information in the model. After trying several cutoff thresholds and examining model fit, we  removed the 28 observations that are outside the middle 95{\%} of the data. The 
following models are fit on this reduced dataset of 727 observations.

\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{netincome_histograms.pdf}
\caption{\textsl{\small Histograms of Net Income, with full data and the central 95{\%} of data displayed. The values very far from the center are of concern when modeling.}}
\label{figure:netincome_hist}
\end{figure}

\begin{table}[h!]
\centering
\begin{tabular}{cc}
\bfseries \underline{Elastic Net} & \bfseries \underline{Forward~Selection}
\csvreader[head to column names]{netincome_top5names.csv}{}%
{\\\elasticnet & \forward}%
\end{tabular}
\caption{\textsl{\small Modeling of Net Income, Top 5 features for both methods}}
\label{table:netincome_top5}
\end{table}

The results of the best 5 variable model are shown in Table \ref{table:netincome_top5}, with a RMSE of 5790.35 for Elastic Net. 
For interpretation of the RMSE it is important to keep in mind the scale for \textit{Net Income} is much larger than that of log 
productivity. However, in this case adding variables has caused a minimal decrease in the error. The cross-validation plot for 
\textit{Net Income} show wider error bars throughout the range of model sizes and just using the average 
\textit{Net Income} would predict nearly as well as any other model. Since here $\lambda_{1se}$ only includes the intercept, we 
chose the optimal Elastic Net model to be at $\lambda_{min}$, which included 9 variables and had a RMSE of 5768.30.

\begin{wrapfigure}{l}{0.6\textwidth}
\vspace*{-0.5cm}
\includegraphics[width=0.6\textwidth]{elastic_cv_netincome.pdf}
\setlength{\belowcaptionskip}{-10pt}
\caption{\textsl{\small Elastic Net Cross-validation plot for Net Income. The plot shows a much smaller improvement in prediction error.}}
\label{figure:netincome_opt_e}
\end{wrapfigure}

For Forward Selection the 5 variable model and full model had $R^2$ values of 0.15 and 0.19, respectively. The full Forward 
Selection model included 9 variables, with all but two overlapping with the Elastic Net choices. The coefficients estimated for 
both models can be found in Table \ref{table:netincome_full} in the Appendix. 

Since the full models are not much larger than the 5 variable models, the small improvements are not surprising. Diagnostics of 
the linear fit of the optimal Elastic Net and Forward Selection models (plots shown in Figure 
\ref{figure:resids_netincome} in the Appendix) do not follow assumptions as closely as for the Production models, but are not so 
concerning as to disqualify the results. On the whole, the results from the various plots and diagnostics suggest that the 
relationships found for Net Income are worth investigating, but that a linear relationship provides little predictive power.

\begin{figure}[h!]
\centering
\includegraphics[width=0.55\textwidth]{forward_nvars_netincome.pdf}
\setlength{\belowcaptionskip}{-10pt}
\caption{\textsl{\small Forward Selection plot for Net Income, which includes a small number of variables.}}
\label{figure:netincome_opt_f}
\end{figure}

\subsubsection{Number of Workers Supported}

\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{worker_histograms.pdf}
\caption{\textsl{\small Histograms of Workers, showing the proportion of zeroes, farms with nonzero workers, and the log transformed number of workers. The log transformation helps remove the skewness, but some bi-modality still remains.}}
\label{figure:workers_hist}
\end{figure}

The first plot in Figure \ref{figure:workers_hist} shows that a large percentage (65{\%}) of the farms are not able to support any 
workers. For this reason, we divided up the modeling task for \textit{Number of Workers Supported}. First, we used logistic regression to model 
the binary variable of whether a farm supports zero or more than zero workers. Secondly, we used linear regression to model 
the quantity of workers for just those 264 farms with a positive number of workers supported.

\begin{table}[h!]
\centering
\setlength{\tabcolsep}{20pt}
\begin{tabular}{cc}
\bfseries \underline{Elastic Net} & \bfseries \underline{Forward~Selection}
\csvreader[head to column names]{workers_binary_top5names.csv}{}%
{\\\elasticnet & \forward}%
\end{tabular}
\caption{\textsl{\small Modeling of Binary Workers , Top features for both methods}}
\label{table:workers_binary_top5}
\end{table}

\begin{table}[h!]
\centering
\setlength{\tabcolsep}{20pt}
\begin{tabular}{cc}
\bfseries \underline{Elastic Net} & \bfseries \underline{Forward~Selection}
\csvreader[head to column names]{workers_nonzero_top5names.csv}{}%
{\\\elasticnet & \forward}%
\newline
\end{tabular}
\caption{\textsl{\small Modeling of Nonzero Workers, Top 5 features for both methods}}
\label{table:workers_nonzero_top5}
\end{table}

The top 5 variable models for logistic regression and linear regression can be found in Table \ref{table:workers_binary_top5} 
and Table \ref{table:workers_nonzero_top5}, respectively. 
Sometimes Elastic Net will simultaneously choose to eliminate multiple variables, in this case there is not a 5 
variable model so we show the 6 variable model for Elastic Net. For logistic regression we can measure performance with the 
misclassification rate, which on average was 0.31 on the test set for Elastic Net, compared to the 0.35 misclassification rate we 
would achieve if we simply predicted the majority class, zero workers, every time.  The full Elastic Net logistic regression model 
had 6 variables and the same 0.31 misclassification rate. For Elastic Net linear regression, the 5 variable model resulted in a 
RMSE of 0.46 and the full model also included 5 variables, and therefore the same RMSE.

\begin{figure}[h]
\begin{subfigure}[b]{0.5\textwidth}
\includegraphics[width=\textwidth]{elastic_cv_workers_binary.pdf}
\caption{\textsl{\small Cross-validation plot for Elastic Net}}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.5\textwidth}
\includegraphics[width=\textwidth]{forward_nvars_workers_binary.pdf}
\caption{Forward Selection plot}
\end{subfigure}
\caption{\textsl{\small Workers Binary Variable Selection. The Elastic Net plot shows ideal predictive performance when including 5-25 variables. The computation for logistic regression Forward Selection stops at the optimal value so the rest of the plot is not included as before.}}
\label{figure:workers_binary_opt}
\end{figure}

For logistic regression Forward Selection the 5 variable model had a misclassification rate of 0.26, which is measured on the 
original dataset, not a test set. The optimal Forward Selection logistic regression chose a model of size 7 also with a 
misclassification rate of 0.26. For linear regression, the 5 variable Forward Selection model had a $R^2$ of 0.34 and the 
optimal model had a $R^2$ of 0.34 using 8 variables. 

In the Appendix, the coefficients estimated for logistic regression models can be found in Table \ref{table:workers_binary_full} 
and the coefficients estimated for linear regression models can be found in Table \ref{table:workers_nonzero_full}. The fairly 
low misclassification rate coupled with low $R^2$ show that there is a fairly clear divide between farms that can or cannot 
support workers, but the specific number of workers is much harder to predict. Several variables are found to be most helpful 
for both the logistic regression and linear regression models, but there is still a fair bit of difference. One common theme of the 
variables chosen here is that many are related to cattle production.

\begin{figure}[h]
\begin{subfigure}[b]{0.5\textwidth}
\includegraphics[width=\textwidth]{elastic_cv_workers_nonzero.pdf}
\caption{\textsl{\small Cross-validation plot for Elastic Net}}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.5\textwidth}
\includegraphics[width=\textwidth]{forward_nvars_workers_nonzero.pdf}
\caption{Forward Selection plot}
\end{subfigure}
\caption{\textsl{\small Nonzero Workers Variable Selection. Both plots show that adding variables does little to improve performance, highlighting a lack of linear relationship.}}
\label{figure:workers_nonzero_opt}
\end{figure}

\subsection{Invasive Species}

The analysis of \textit{Invasive Species} follows much the same pattern as for \textit{Number of Workers Supported}. Again, we have a large 
number of zeros in the outcome variable, farms with zero percent of their land covered by invasive species. We first modeled 
this binary variable using logistic regression and then performed linear regression on the log transformed values for the farms 
that have a percent coverage greater than zero. As can be seen in the far right plot of Figure \ref{figure:invasive_hist} there are 
two very small values (0.005{\%} and 0.08{\%} prior to taking the log). They effect the fit of the model and are negligibly above 
zero so we removed them from the linear model, leaving 155 farms for modeling.

\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{invasive_histograms.pdf}
\caption{Histogram of Invasive Species. To the left: the proportion of zeroes, in the middle: farms with greater than zero, and to the right: the log transformed percent of invasive species. The two very small values on the right plot are removed.}
\label{figure:invasive_hist}
\end{figure}

\begin{table}[h]
\centering
\setlength{\tabcolsep}{20pt}
\begin{tabular}{cc}
\bfseries \underline{Elastic Net} & \bfseries \underline{Forward~Selection}
\csvreader[head to column names]{invasive_binary_top5names.csv}{}%
{\\\elasticnet & \forward}%
\end{tabular}
\caption{\textsl{\small Modeling of Binary Invasive , Top features for each method}}
\label{table:invasive_binary_top5}
\end{table}

\begin{table}[h]
\centering
\setlength{\tabcolsep}{20pt}
\begin{tabular}{cc}
\bfseries \underline{Elastic Net} & \bfseries \underline{Forward~Selection}
\csvreader[head to column names]{invasive_nonzero_top5names.csv}{}%
{\\\elasticnet & \forward}%
\end{tabular}
\caption{\textsl{\small Modeling of Nonzero Invasive , Top 5 features for each method}}
\label{table:invasive_nonzero_top5}
\end{table}

\pagebreak

The top 5 variable models for logistic regression and linear regression can be found in Table \ref{table:invasive_binary_top5} 
and Table \ref{table:invasive_nonzero_top5}, respectively. For Elastic Net logistic regression the misclassification rate was 0.21 
on the test set for the 5 variable model. For the Elastic Net linear regression 5 variable model the RMSE was 0.25 and the optimal model only included 4 variables with the same RMSE.

\begin{figure}[h!]
\begin{subfigure}[b]{0.5\textwidth}
\includegraphics[width=\textwidth]{elastic_cv_invasive_binary.pdf}
\caption{\textsl{\small  Cross-validation plot for Elastic Net}}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.5\textwidth}
\includegraphics[width=\textwidth]{forward_nvars_invasive_binary.pdf}
\caption{\textsl{\small Forward Selection plot}}
\end{subfigure}
\caption{\textsl{\small Invasive Binary Variable Selection. The Elastic Net plot shows adding variables does not improve just predicting the majority class. In Forward Selection the BIC very quickly levels off.}}
\label{figure:invasive_binary_opt}
\end{figure}

\begin{figure}[h!]
\begin{subfigure}[b]{0.5\textwidth}
\includegraphics[width=\textwidth]{elastic_cv_invasive_nonzero.pdf}
\caption{\textsl{\small Cross-validation plot for Elastic Net}}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.5\textwidth}
\includegraphics[width=\textwidth]{forward_nvars_invasive_nonzero.pdf}
\caption{\textsl{\small Forward Selection plot}}
\end{subfigure}
\caption{\textsl{\small Nonzero Invasive Variable Selection. Similar to Figure \ref{figure:invasive_binary_opt} the lack of benefit from adding additional variables demonstrates a weak linear relationship.}}
\label{figure:invasive_nonzero_opt}
\end{figure}

For Forward Selection the 5 variable logistic regression model had a misclassification rate of 0.20 on the full dataset. The 5 variable linear regression model had an $R^2$ of 0.30 and the optimal model included 7 variables to increase the $R^2$ 0.35.

The cross-validation and forward selection plots in Figure \ref{figure:invasive_binary_opt} show that there was little gained from 
adding variables to the logistic regression model. The optimal Elastic Net model only included the intercept and the optimal 
Forward Selection model still only included 5 variables. For this reason the misclassification rates are not changed from above. 

Since the percentage of farms with no invasive species is 20{\%} the logistic regression was not able to do much more than just 
predict zero for all farms. This combined with the low performance of the linear model suggests that invasive species coverage cannot be well explained by the variables considered here. That said, more than seen in previous outcome variables, the canton variables seem to be particularly important for \textit{invasive species}. Similar variables were chosen for Elastic Net and Forward Selection in logistic regression, but not for linear regression.

\subsection{Land use choices}

The analysis of land use choice requires a slightly different approach than used previously since the outcome variable is a 
categorical variable, with six different classifications of land use (each measured as a percentage of total land area):
\begin{itemize}
	\item{Permanent Crops}
	\item{Temporary Crops}
	\item{Fallow Land}
	\item{Tilled Land}
	\item{Pasture}
	\item{Brush}
\end{itemize}

While not a perfect ordering, the classifications can generally be thought of progressing from land that has been most worked 
by the farmer to least worked. We visualized the land use data using a parallel coordinate plot (Figure 
\ref{figure:landuse_plot}) where each line represents a farm and the height represents the percent of land used for that 
category. Darker or thicker line areas indicate more farms falling along that path. We can see that there are farms primarily 
used for each of the different land use categories, but many more so for pasture, permanent crops and brush. If there were 
many horizontal, or near horizontal, lines on the graph that would indicate even distribution of land across multiple categories, 
but there is not much evidence of that in this plot.

\begin{figure}[h!]
\centering
\includegraphics[width = 0.95\textwidth]{landuse_parplot.pdf}
\caption{\textsl{\small Parallel Coordinate Plot of Land Use. There appear to be different types of farms, some primarily permanent crops, some primarily temporary crops, etc. The land use categories pasture, permanent crops and brush appear to be the most prevalent.}}
\label{figure:landuse_plot}
\end{figure}

There are multiple ways that this outcome variable can be formulated for modeling, but we decide to label each farm with its 
highest percentage land use category. There are only 9 such farms labeled as \textit{fallow}, which is too small for modeling so we 
remove them from the consideration. The remaining categories and the number of farms are shown in Table \ref{table:landuse_cat}.

\begin{table}[h]
\centering
\begin{tabular}{lc}
Majority Category    & Number of Farms \\
\hline
Perm    & 209             \\
Temp    & 50              \\
Till    & 36              \\
Pasture & 271             \\
Brush   & 189            
\end{tabular}
\caption{\textsl{\small Landuse Category Count}}
\label{table:landuse_cat}
\end{table}  	

Using this derived categorical variable we used multinomial logistic regression with Elastic Net to build a model, which found an 
optimal set of coefficients for each category. The results from the 5 variable model, which had a misclassification rate of 0.39, 
are shown in Table \ref{table:landuse_top5}. Different categories may have varying number of coefficients and \textit{Perctill} 
had no variables included at this lambda level. As might be expected, presence of permanent crops predicted \textit{Percperm}, 
presence of transitory crops predicted \textit{Perctemp}, etc., but the specific crops found most predictive may be of interest.

\begin{table}[h]
\centering
\setlength{\tabcolsep}{20pt}
\begin{tabular}{cc}
\bfseries \underline{Percperm} & \bfseries \underline{Perctemp}%
\csvreader[head to column names]{landuse_top5names.csv}{}%
{\\\percperm & \perctemp}%
\end{tabular}
\begin{tabular}{ccc}
\bfseries \underline{Percpasture} & \bfseries \underline{Percbrush} & \bfseries \underline{Perctill}%
\csvreader[head to column names]{landuse_top5names.csv}{}%
{\\\percpasture & \percbrush & \perctill}%
\end{tabular}
\caption{\textsl{\small Modeling of Landuse, Top features for each category}}
\label{table:landuse_top5}
\end{table}

\pagebreak

The plot of the Elastic Net cross-validation is shown in Figure \ref{figure:landuse_opt}. The optimal model achieved a 0.36 misclassification rate against the test set on average, which given 5 categories to choose from shows decent predictive strength. In the Appendix, the coefficients estimated for the optimal model can be found in Tables \ref{table:landuse_full_a} and 
\ref{table:landuse_full_b}. 

\begin{figure}[!tbp]
\includegraphics[width=\textwidth]{elastic_cv_landuse.pdf}
\caption{\textsl{\small Land Use Cross-validation plot for Elastic Net. The Elastic Net model shows a large improvement over just predicting the majority class. Since there are now 5 categories, such a strategy would result in a 0.65 misclassification rate.}}
\hfill
\label{figure:landuse_opt}
\end{figure}

\section{Statistical Methods} \label{stat_methods}

\subsection{Generalized Linear Models}

\[\bY = \bX\beta + \epsilon \tag{1}\]

Standard Linear Regression can be represented in matrix form as seen in equation 1 above, When there are $n$ observations and $p$ predictor variables,  $\bY$ is a $n \times 1$ vector of the outcome variable, $\bX$ is a $n \times p$ matrix of predictor variables, $\bbeta$ is a $p \times 1$ vector of variable coefficients and $\epsilon$ is the error 
term. The standard linear model works best when the outcome variable $\bY$ has a normal distribution, and therefore takes 
continuous values. When the outcome variable is continuous, but not normal shaped (e.g., skewed like the productivity 
data) it can be possible to transform the data by taking the logarithm or something similar. However, when the outcome 
variable is discrete (such as binary labels of 1 and 0 denoting absence/presence of a feature) a further modification must be made. The outcome variable is clearly no longer normally distributed, as it is not even continuous. Without modification we could get predicted values below 0, above 1 or somewhere in between, none of which make much sense.

This calls for the use of logistic regression, a type of generalized linear model \cite{dobson}, in which we perform a logit transformation as seen in equation 2 below so that the 
$\bX\beta$ can still be mapped to a continuous scale. In some respects this is a computational concern, but it also changes the 
way coefficients can be interpreted. For example, instead of a one unit change in $X_1$ predicting a $\beta_1$ change in the predicted $Y$, 
in this case it predicts a $\beta_1$ change in the log odds of $Y$.

\[log \frac{\Pr(Y = 1)}{\Pr(Y = 0)} = \bX\beta \tag{2}\]

Equation 2 can be rewritten as below in equation 3, which gives the predicted probability of an observation being class 1.

\[ \Pr(Y = 1) = \frac{e^{\bX\beta}}{1 + e^{\bX\beta}} \tag{3}\]

This can be extended beyond binary variables to categorical variables when there are $K$ classes, referred to as Multinomial 
Logistic Regression. Now to calculate the probability of class $k$, we use equation 4 below.

\[ \Pr(Y = k) = \frac{e^{\bX\beta_k}}{\sum_{l=1}^{K-1} e^{\bX\beta_l}} \tag{4}\]

\subsection{Performance Measures}

There are many measures of fit for linear models. When there are many possible predictor variables, care must be taken 
to use appropriate measures, as some measures will favor just adding all of the variables to the model. For example, if we 
aim to minimize the mean square error adding more predictor variables to the model will always be encouraged. Since that is 
not desired, measurements like Bayesian Information Criterion (BIC) can be used. BIC is a combination of how well the model 
fits the data and a penalty term for the number of predictor variables included in the model. The goal is to minimize BIC, that is 
the model with the best balance of small size and goodness of fit. BIC is chosen over other potential measurements because it 
puts a large penalty on the inclusion of additional variables.

Another approach is to use cross-validation. In this technique the dataset is first split into $k$ equally sized sets. Then a 
model is fit using $k - 1$ of the sets (training sets) and evaluated on the remaining 1 (test) set. This is repeated $k$ times, 
each time reserving a different 1 test set, and then results across the $k$ runs are averaged. The benefit of this is that model 
building and model evaluating are happening on different portions of the data, so we can distinguish if the model is picking up 
on generalizable patterns or just random noise. Using cross-validation the average test set mean square error is an 
appropriate measure of model fit. We can also capture the standard deviation across the $k$ runs to measure variability, which is shown in the error bars of the Elastic Net cross-validation plots.

\subsection{Best Subset and Forward Selection}

The essential goal of variable selection is to find the best combination of predictor variables to explain the outcome variable. As 
discussed above, when we have many possible predictors we often want to put a constraint on the problem so that all variables 
are not included. Such a constraint might be limiting the number of variables included or that the model found can generalize to 
other data. Best subset selection, the most natural, but computationally difficult way is to try all possible combinations of 
variables and select the best fitting combination. However, when the number of variables, $p$, is large this quickly becomes 
infeasible, as there are $2^p$ possible combinations. 

One approach to tackle the computational complexity discussed above is to restrict the search for the optimal number of 
predictor variables, which is what Forward Selection does. In this algorithm, we start with an empty model and iteratively add a 
new variable at each stage that most increases the fit. This procedure can work well, but may not find the optimal solution. As 
an example, consider a case where $X_1$ is the single most predictive variable, but the combination of $X_2$ and $X_3$ is 
the best two variable combination. The algorithm will first add $X_1$, but then regardless whether it adds $X_2$ or $X_3$ next, 
it will have found a suboptimal solution. In general, we can decide to stop adding variables once we have reached an optimal 
performance measure like BIC or cross-validation test error.

\pagebreak

\subsection{Regularized Regression}

	\[\min_{\beta}  \|\bY - \bX\beta\|_{2}^{2} \tag{linear model}\]
	\[\min_{\beta}  \|\bY - \bX\beta\|_{2}^{2} + \lambda||\beta||_2^2 \tag{ridge regression}\]
	\[\min_{\beta}  \|\bY - \bX\beta\|_{2}^{2} + \lambda||\beta|| \tag{LASSO}\]

The above notation of $||\cdot||_{2}^{2}$ and  $||\cdot||$ are defined in general as: $||\bX||_{2}^{2} = x_1^2 + x_2^2 + ... + 
x_n^2$ and $||\bX|| = |x_1| + |x_2| + ... + |x_n|$. As shown in the top equation of the standard linear model, we try to find the $\beta$, that is a vector of coefficients,  
that minimizes the squared difference between the true $\bY$ and the predicted $\hat{\bY}$ (which is $\bX\beta$). In 
regularized regression we do the same thing, but also add a second term that we look to simultaneously minimize. This second 
term adds a penalty scaled by $\lambda$ for increasing values of $\beta$, so the two terms must be balanced. The optimal model will find a balance 
between fitting the outcome variable closely, but not having too large of coefficient values. The difference between Ridge 
regression and LASSO is how we add up the coefficients. In Ridge Regression the coefficients are squared and then summed, 
in LASSO we take the absolute value of 
the coefficients and then sum them. LASSO will encourage most of the coefficients to go to zero, thus only including a small 
number of terms in the model. Ridge regression will encourage the coefficient values to be spread out among predictor 
variables, leaving all of the variables in the model, but helping to offset negative effects of correlated predictor variables. 

\[\min_{\beta}  \|\bY - \bX\beta\|_{2}^{2} + \lambda[(1 - \alpha)||\beta||_2^2 + \alpha||\beta||] \tag{Elastic Net}\]

The technique that is used in this analysis is a combination of the Ridge and LASSO penalties, called Elastic Net. As can be seen in the equation above both the square of the coefficients and the absolute value of the coefficients is included, with the contribution of each controlled by the size of alpha ($\alpha$) which takes a value between 0 and 1. Elastic Net, thus combines the favorable properties of Ridge and LASSO, in that it can achieve both sparse models and can handle correlated predictor variables. Both the $\lambda$ and the $\alpha$ can be set using cross-validation (as discussed above) to appropriate values for the particular dataset. In each Elastic Net cross-validation plot found in the Results section, the specific $\alpha$ used is labeled at the top of the graph.

\section{Limitations and Future Work}
There are a few key considerations that should be kept in mind when interpreting this analysis. The first is that relationships discovered in this analysis are correlational in nature and cannot be assumed to be causal. Just because farms with a higher coverage of invasive species have lower productivity does not necessarily mean the invasive species causes lower productivity. It could be that lower productivity causes higher invasive species coverage. Or there could other factors not captured in the model that influence both productivity and invasive species. In order to determine causality, relationships of interest should be tested in a designed experiment. 

Secondly, p-values and confidence intervals for coefficients were intentionally not included in the analysis. In standard 
regression analysis we pre-specify the model and then test which variables are found to be significant. However, when using 
Elastic Net and Forward Selection like we have done here, we do not specify the model ahead of time, but instead let the data 
decide the model. This violates the significance test assumption and can lead to misleadingly small p-values (see Chapter 6 of \cite{hastie}). While there is some recent work (\cite{zhao}) suggesting this may be acceptable under certain assumptions, as well as some advanced techniques to try to adjust for this, it is recommended to view the results in this report as an exploratory analysis rather than definitive evidence.

Future analysis might look to explore better fitting relationships, particularly for the outcome variables that had poor RMSE and 
$R^2$ values. The relationships modeled in this report only considered linear combinations of predictor variables to predict/
explain the outcome variables. Modifications could include adding interaction terms (i.e., $x_1x_2$) or nonlinear terms (i.e., 
$x_1^2$ or binary transformations $x_1 > 10$). Exploring all possible modifications of this type is not computationally feasible, 
but with domain knowledge a subset of theorized relationships could be tested. 

For some of the outcome variables the island was found to be a significant varriable, but it may be more helpful to model each 
of the islands separately. A potential limitation to such an approach would be the small sample sizes for some of the islands, so 
this would be best carried out with a smaller set of potential predictor variables. Lastly, the variables used here primarily 
covered socioeconomic dimensions. The addition of physical and biotic variables may help better predict/explain the outcome 
variables or may change the importance of previously highlighted socioeconomic variables.


\medskip

\small

\begin{thebibliography}{99}

\bibitem{census} Censo De Las Unidades De Producci\'on Agropecuaria De Gal\'apagos, sinagap.agricultura.gob.ec/pdf/censo\_galapagos/cuestionario\_censo\_upa\_galapagos.pdf.

\bibitem{dobson} Dobson, A. J., \& Barnett, A. (2008). An introduction to generalized linear models. CRC press.

\bibitem{friedman} Friedman, J., Hastie, T., \& Tibshirani, R. (2009). glmnet: Lasso and elastic-net regularized generalized linear models. R package version, 1(4).

\bibitem{hastie} Hastie, T., Tibshirani, R., \& Wainwright, M. (2015). Statistical learning with sparsity: the lasso and 
generalizations. CRC press.

\bibitem{lumley} Lumley, T., \& Miller, A. (2009). Leaps: regression subset selection. R package version 2.9. Online at http://CRAN. R-project.org/package= leaps.

\bibitem{miller} Miller, B. W., Breckheimer, I., McCleary, A. L., Guzm\'an-Ramirez, L., Caplow, S. C., Jones-Smith, J. C., \& Walsh, S. J. (2010). Using stylized agent-based models for population environment research: a case study from the Gal\'apagos Islands. Population and environment, 31(6), 401-426.

\bibitem{valdivia}Valdivia, G., Wolford, W., \& Lu, F. (2014). Border crossings: New geographies of protection and production in the Gal\'apagos Islands. Annals of the Association of American Geographers, 104(3), 686-701.

\bibitem{walsh} Walsh S.J., Mena C.F. (2013) Perspectives for the Study of the Gal\'apagos Islands: Complex Systems and Human-Environment Interactions. In: Walsh S., Mena C. (eds) Science and Conservation in the Gal\'apagos Islands. Social and Ecological Interactions in the Gal\'apagos Islands, vol 1. Springer, New York, NY

\bibitem{zhao} Zhao, S., Shojaie, A., \& Witten, D. (2017). In Defense of the Indefensible: A Very Naive Approach to High-Dimensional Inference. arXiv preprint arXiv:1705.05543.

\bibitem{zou} Zou, H., \& Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2), 301-320.




\end{thebibliography}

\newpage

\section{Appendix}

\begin{table}[h!]
\npdecimalsign{.}
\nprounddigits{4}
\begin{tabular}{ln{1}{4}n{1}{4}}\hline%
\bfseries \underline{Variable} & \bfseries \underline{Elastic Net} & \bfseries \underline{Forward~Selection}
\csvreader[head to column names]{fullcoeflist_production.csv}{}%
{\\\variable & \elastic & \forward}%
\\\hline
\end{tabular}
\caption{\textsl{\small Full coefficient list for Production model}}
\label{table:prod_full}
\end{table}

\begin{figure}[h]
\includegraphics[width = \textwidth]{resids_production.pdf}
\caption{\textsl{\small Diagnostic Residual plots for Production. The plots on the left show the predicted values vs. the residuals and both demonstrate a desirable lack of pattern. The plots of the right examine the normality of the residuals, both staying close to the desired straight diagonal pattern.}}
\label{figure:resids_prod}
\end{figure}


\begin{table}[h!]
\npdecimalsign{.}
\nprounddigits{4}
\begin{tabular}{ln{4}{4}n{4}{4}}\hline%
\bfseries \underline{Variable} & \bfseries \underline{Elastic Net} & \bfseries \underline{Forward~Selection}
\csvreader[head to column names]{fullcoeflist_netincome.csv}{}%
{\\\variable & \elastic & \forward}%
\\\hline
\end{tabular}
\caption{\textsl{\small Full coefficient list for Net Income model}}
\label{table:netincome_full}
\end{table}

\begin{figure}[h]
\includegraphics[width = \textwidth]{resids_netincome.pdf}
\caption{\textsl{\small Diagnostic Residual plots for Net Income. The plots of the predicted values vs. the residuals both demonstrate a desirable lack of pattern. The plots examining the normality of the residuals do not follow a diagonal line as closely as would be hoped, but are not an extreme departure.}}
\label{figure:resids_netincome}
\end{figure}

\begin{table}[h!]
\npdecimalsign{.}
\nprounddigits{4}
\begin{tabular}{ln{1}{4}n{1}{4}}\hline%
\bfseries \underline{Variable} & \bfseries \underline{Elastic Net} & \bfseries \underline{Forward~Selection}
\csvreader[head to column names]{fullcoeflist_workers_binary.csv}{}%
{\\\variable & \elastic & \forward}%
\\\hline
\end{tabular}
\caption{\textsl{\small Full coefficient list for Binary Workers model}}
\label{table:workers_binary_full}
\end{table}

\begin{table}[h!]
\npdecimalsign{.}
\nprounddigits{4}
\begin{tabular}{ln{1}{4}n{1}{4}}\hline%
\bfseries \underline{Variable} & \bfseries \underline{Elastic Net} & \bfseries \underline{Forward~Selection}
\csvreader[head to column names]{fullcoeflist_workers_nonzero.csv}{}%
{\\\variable & \elastic & \forward}%
\\\hline
\end{tabular}
\caption{\textsl{\small Full coefficient list for Nonzero Workers model}}
\label{table:workers_nonzero_full}
\end{table}

\begin{figure}[h]
\includegraphics[width = \textwidth]{resids_workers_nonzero.pdf}
\caption{\textsl{\small Diagnostic Residual plots for Nonzero workers. The plots of predictions versus residuals shows points bunch at the left end, but the QQ plot closely follows a diagonal line.}}
\label{figure:resids_workers_nonzero}
\end{figure}

\begin{table}[h!]
\npdecimalsign{.}
\nprounddigits{4}
\begin{tabular}{ln{1}{4}n{1}{4}}\hline%
\bfseries \underline{Variable} & \bfseries \underline{Elastic Net} & \bfseries \underline{Forward~Selection}
\csvreader[head to column names]{fullcoeflist_invasive_binary.csv}{}%
{\\\variable & \elastic & \forward}%
\\\hline
\end{tabular}
\caption{\textsl{\small Full coefficient list for Binary Invasive model}}
\label{table:workers_invasive_full}
\end{table}

\begin{table}[h!]
\npdecimalsign{.}
\nprounddigits{4}
\begin{tabular}{ln{1}{4}n{1}{4}}\hline%
\bfseries \underline{Variable} & \bfseries \underline{Elastic Net} & \bfseries \underline{Forward~Selection}
\csvreader[head to column names]{fullcoeflist_invasive_nonzero.csv}{}%
{\\\variable & \elastic & \forward}%
\\\hline
\end{tabular}
\caption{\textsl{\small Full coefficient list for Nonzero Invasive model}}
\label{table:invasive_nonzero_full}
\end{table}

\begin{figure}[h]
\includegraphics[width = \textwidth]{resids_invasive_nonzero.pdf}
\caption{\textsl{\small Diagnostic Residual plots for Nonzero Invasive. Due to only a few variables being included in these models and some of them being discrete, the predictions occur at a limited number of distinct points.}}
\label{figure:resids_invasive_nonzero}
\end{figure}

\begin{table}[h!]
\npdecimalsign{.}
\nprounddigits{4}
\begin{tabular}{ln{1}{4}n{1}{4}n{1}{4}}\hline%
\bfseries \underline{Variable} & \bfseries \underline{Percperm} & \bfseries \underline{Perctemp} & \bfseries \underline{Perctill}%
\csvreader[head to column names]{fullcoeflist_landuse_a.csv}{}%
{\\\variable & \percperm & \perctemp & \perctill}%
\\\hline
\end{tabular}
\caption{\textsl{\small Full coefficient list for Landuse model}}
\label{table:landuse_full_a}
\end{table}

\begin{table}[h!]
\npdecimalsign{.}
\nprounddigits{4}
\begin{tabular}{ln{1}{4}n{1}{4}}\hline%
\bfseries \underline{Variable} & \bfseries \underline{Percpasture} & \bfseries \underline{Percbrush}%
\csvreader[head to column names]{fullcoeflist_landuse_b.csv}{}%
{\\\variable & \percpasture & \percbrush}%
\\\hline
\end{tabular}
\caption{\textsl{\small Full coefficient list for Landuse model (cont.)}}
\label{table:landuse_full_b}
\end{table}

\end{document}



